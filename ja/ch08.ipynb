{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1e1YwuFtZd1t",
      "metadata": {
        "editable": true,
        "id": "1e1YwuFtZd1t",
        "tags": []
      },
      "source": [
        "# 第8章: ニューラルネット\n",
        "\n",
        "第7章で取り組んだポジネガ分類を題材として、ニューラルネットワークで分類モデルを実装する。なお、この章ではPyTorchやTensorFlow、JAXなどの深層学習フレームワークを活用せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "Flj9e5wrrt2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ca0ec9a-236c-457b-ee7a-9fbbbb764b67"
      },
      "id": "Flj9e5wrrt2L",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff",
      "metadata": {
        "id": "b03603ee-a54b-4b93-97a2-888f5e3feeff"
      },
      "source": [
        "## 70. 単語埋め込みの読み込み\n",
        "\n",
        "事前学習済み単語埋め込みを活用し、$|V| \\times d_\\rm{emb}$ の単語埋め込み行列$\\pmb{E}$を作成せよ。ここで、$|V|$は単語埋め込みの語彙数、$d_\\rm{emb}$は単語埋め込みの次元数である。ただし、単語埋め込み行列の先頭の行ベクトル$\\pmb{E}_{0,:}$は、将来的にパディング（`<PAD>`）トークンの埋め込みベクトルとして用いたいので、ゼロベクトルとして予約せよ。ゆえに、$\\pmb{E}$の2行目以降に事前学習済み単語埋め込みを読み込むことになる。\n",
        "\n",
        "もし、Google Newsデータセットの[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)（300万単語・フレーズ、300次元）を全て読み込んだ場合、$|V|=3000001, d_\\rm{emb}=300$になるはずである（ただ、300万単語の中には、殆ど用いられない稀な単語も含まれるので、語彙を削減した方がメモリの節約になる）。\n",
        "\n",
        "また、単語埋め込み行列の構築と同時に、単語埋め込み行列の各行のインデックス番号（トークンID）と、単語（トークン）への双方向の対応付けを保持せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Google Newsの学習済み単語ベクトルをダウンロード・ロード\n",
        "w2v_model = api.load('word2vec-google-news-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6zmefrJVUx6g",
        "outputId": "a755954f-b518-498b-82d6-9e36e57a7454"
      },
      "id": "6zmefrJVUx6g",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# パラメータ\n",
        "embedding_dim = 300  # demb\n",
        "vocab_limit = 50000  # 使用する語彙数（+1して<PAD>の分も確保）\n",
        "\n",
        "print('vocab size = ',len(w2v_model.index_to_key) + 1)\n",
        "\n",
        "# 語彙を絞って使用（vocab_limit - 1）個取り出し（1個はPAD用）\n",
        "vocab_words = w2v_model.index_to_key[:vocab_limit - 1]\n",
        "\n",
        "# 単語からインデックスへの辞書（PAD: 0）\n",
        "word_to_id = {\"<PAD>\": 0}\n",
        "id_to_word = {0: \"<PAD>\"}\n",
        "\n",
        "# 単語埋め込み行列の初期化（1行目は0ベクトル）\n",
        "embedding_matrix = np.zeros((vocab_limit, embedding_dim), dtype=np.float32)\n",
        "\n",
        "# 2行目以降を埋める\n",
        "for i, word in enumerate(vocab_words, start=1):\n",
        "    embedding_matrix[i] = w2v_model[word]\n",
        "    word_to_id[word] = i\n",
        "    id_to_word[i] = word\n",
        "\n",
        "# PyTorchのテンソルに変換\n",
        "E = torch.tensor(embedding_matrix)\n",
        "\n",
        "print(f\"Embedding matrix shape: {E.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pawCzPtdP9G",
        "outputId": "8653488a-31c7-401b-d75f-24095767450d"
      },
      "id": "6pawCzPtdP9G",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab size =  3000001\n",
            "Embedding matrix shape: torch.Size([50000, 300])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e",
      "metadata": {
        "id": "c45bc5ba-4a83-493a-a78e-04aa48f3db2e"
      },
      "source": [
        "## 71. データセットの読み込み\n",
        "\n",
        "[General Language Understanding Evaluation (GLUE)](https://gluebenchmark.com/) ベンチマークで配布されている[Stanford Sentiment Treebank (SST)](https://dl.fbaipublicfiles.com/glue/data/SST-2.zip) をダウンロードし、訓練セット（train.tsv）と開発セット（dev.tsv）のテキストと極性ラベルと読み込み、全てのテキストをトークンID列に変換せよ。このとき、単語埋め込みの語彙でカバーされていない単語は無視し、トークン列に含めないことにせよ。また、テキストの全トークンが単語埋め込みの語彙に含まれておらず、空のトークン列となってしまう事例は、訓練セットおよび開発セットから削除せよ（このため、第7章の実験で得られた正解率と比較できなくなることに注意せよ）。\n",
        "\n",
        "事例の表現方法は任意でよいが、例えば\"contains no wit , only labored gags\"がネガティブに分類される事例は、次のような辞書オブジェクトで表現すればよい。\n",
        "\n",
        "```\n",
        "{'text': 'contains no wit , only labored gags',\n",
        " 'label': tensor([0.]),\n",
        " 'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])}\n",
        "```\n",
        "\n",
        "この例では、`text`はテキスト、`label`は分類ラベル（ポジティブなら`tensor([1.])`、ネガティブなら`tensor([0.])`）、`input_ids`はテキストのトークン列をID列で表現している。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install scikit-learn\n",
        "!pip install numpy\n",
        "!wget https://dl.fbaipublicfiles.com/glue/data/SST-2.zip\n",
        "!unzip SST-2.zip"
      ],
      "metadata": {
        "id": "cp5tW3x6iCfA"
      },
      "id": "cp5tW3x6iCfA",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# SST-2の読み込み\n",
        "train_df = pd.read_csv(\"SST-2/train.tsv\", sep='\\t')\n",
        "dev_df = pd.read_csv(\"SST-2/dev.tsv\", sep='\\t')\n",
        "\n",
        "# 変換関数の定義\n",
        "def convert_to_tensorized_data(df, word_to_id):\n",
        "    processed = []\n",
        "    for _, row in df.iterrows():\n",
        "        text = row['sentence']\n",
        "        label = torch.tensor([row[\"label\"]], dtype=torch.float32)  # tensor([0.]) or tensor([1.])\n",
        "        tokens = text.split()\n",
        "        input_ids = [word_to_id[token] for token in tokens if token in word_to_id]\n",
        "        if input_ids:  # 空リストでなければ処理\n",
        "            input_tensor = torch.tensor(input_ids)\n",
        "            processed.append({\n",
        "                'text': text,\n",
        "                'label': label,\n",
        "                'input_ids': input_tensor\n",
        "            })\n",
        "    return processed\n",
        "\n",
        "# 実行\n",
        "train_tensor_data = convert_to_tensorized_data(train_df, word_to_id)\n",
        "dev_tensor_data = convert_to_tensorized_data(dev_df, word_to_id)\n",
        "\n",
        "# 確認（先頭の1件を表示）\n",
        "print(\"Train size:\", len(train_tensor_data))\n",
        "print(\"Dev size:\", len(dev_tensor_data))\n",
        "print(\"Example (train):\", train_tensor_data[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJyzWkiAh60r",
        "outputId": "5f356409-08ca-404f-a85d-08aae0d498bb"
      },
      "id": "XJyzWkiAh60r",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size: 65018\n",
            "Dev size: 872\n",
            "Example (train): {'text': 'hide new secretions from the parental units ', 'label': tensor([0.]), 'input_ids': tensor([ 5785,    66,    18,    12, 15095,  1594])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca",
      "metadata": {
        "id": "29dfe527-a08c-48fa-b9b4-0acebea36bca"
      },
      "source": [
        "## 72. Bag of wordsモデルの構築\n",
        "\n",
        "単語埋め込みの平均ベクトルでテキストの特徴ベクトルを表現し、重みベクトルとの内積でポジティブ及びネガティブを分類するニューラルネットワーク（ロジスティック回帰モデル）を設計せよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# 平均埋め込みを使ったロジスティック回帰モデル\n",
        "class MeanEmbeddingLogisticRegression(nn.Module):\n",
        "    def __init__(self, embedding_matrix):\n",
        "        super().__init__()\n",
        "        # 事前学習済みの埋め込み行列を固定で読み込む\n",
        "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True, padding_idx=0)\n",
        "        self.linear = nn.Linear(embedding_matrix.size(1), 1)  # 入力次元 = 埋め込みの次元数\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        \"\"\"\n",
        "        input_ids: (batch_size, seq_len)\n",
        "        \"\"\"\n",
        "        # 埋め込み取得: (batch_size, seq_len, emb_dim)\n",
        "        embeddings = self.embedding(input_ids)\n",
        "\n",
        "        # マスクして平均（PAD=0 を除く）\n",
        "        mask = (input_ids != 0).unsqueeze(-1)  # (batch_size, seq_len, 1)\n",
        "        masked_embeddings = embeddings * mask  # PAD部分を0にする\n",
        "        sum_embeddings = masked_embeddings.sum(dim=1)\n",
        "        lengths = mask.sum(dim=1).clamp(min=1)  # 0除算防止\n",
        "        avg_embeddings = sum_embeddings / lengths  # (batch_size, emb_dim)\n",
        "\n",
        "        # 線形層＋シグモイド\n",
        "        logits = self.linear(avg_embeddings)  # (batch_size, 1)\n",
        "        probs = torch.sigmoid(logits).squeeze(1)  # (batch_size)\n",
        "        return probs\n"
      ],
      "metadata": {
        "id": "wGljH5YDiTZ0"
      },
      "id": "wGljH5YDiTZ0",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "72385c44-ceab-4d62-a4df-3023e15a37e2",
      "metadata": {
        "id": "72385c44-ceab-4d62-a4df-3023e15a37e2"
      },
      "source": [
        "## 73. モデルの学習\n",
        "\n",
        "問題72で設計したモデルの重みベクトルを訓練セット上で学習せよ。ただし、学習中は単語埋め込み行列の値を固定せよ（単語埋め込み行列のファインチューニングは行わない）。また、学習時に損失値を表示するなど、学習の進捗状況をモニタリングできるようにせよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# 学習用データ（辞書のリスト）を PyTorch Dataset に変換\n",
        "class SSTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]['input_ids'], self.data[idx]['label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# モデル初期化\n",
        "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "model = MeanEmbeddingLogisticRegression(embedding_tensor)\n",
        "model.embedding.weight.requires_grad = False\n",
        "\n",
        "# 損失関数と最適化手法\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 学習ループ（DataLoaderを使わない）\n",
        "for epoch in range(10):\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "    for item in train_tensor_data:\n",
        "        input_ids = item['input_ids']\n",
        "        label = item['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = input_ids.unsqueeze(0)  # [seq_len] → [1, seq_len]\n",
        "        output = model(input_ids).squeeze()\n",
        "        loss = criterion(output, label.squeeze())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_tensor_data)\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "X7wOB4X4kkAP",
        "outputId": "225cefb2-be59-4c33-e876-d5c12ac035d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "X7wOB4X4kkAP",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4374\n",
            "Epoch [2/10], Loss: 0.4093\n",
            "Epoch [3/10], Loss: 0.4069\n",
            "Epoch [4/10], Loss: 0.4061\n",
            "Epoch [5/10], Loss: 0.4058\n",
            "Epoch [6/10], Loss: 0.4056\n",
            "Epoch [7/10], Loss: 0.4055\n",
            "Epoch [8/10], Loss: 0.4054\n",
            "Epoch [9/10], Loss: 0.4053\n",
            "Epoch [10/10], Loss: 0.4053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# 学習用データ（辞書のリスト）を PyTorch Dataset に変換\n",
        "class SSTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]['input_ids'], self.data[idx]['label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "# 修正された collate_fn（そのまま返すだけ）\n",
        "def collate_no_padding(batch):\n",
        "    input_ids_list, label_list = zip(*batch)\n",
        "    return input_ids_list[0], label_list[0]  # バッチサイズ1想定\n",
        "\n",
        "#PyTorch 用に変換\n",
        "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "# モデル（embeddingはすでに提供されたもの）\n",
        "model = MeanEmbeddingLogisticRegression(embedding_tensor)\n",
        "\n",
        "# 埋め込みは凍結する\n",
        "model.embedding.weight.requires_grad = False\n",
        "\n",
        "# ハイパーパラメータ\n",
        "epochs = 10\n",
        "lr = 1e-3\n",
        "\n",
        "# 損失関数と最適化手法\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# DataLoader 作成（バッチサイズ=1、パディングなし）\n",
        "train_dataset = SSTDataset(train_tensor_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, collate_fn=collate_no_padding)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    model.train()\n",
        "    for input_ids, label in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = input_ids.unsqueeze(0)  # モデルはバッチ次元が必要\n",
        "        output = model(input_ids).squeeze()\n",
        "        loss = criterion(output, label.squeeze())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NLOhfDFgh-h",
        "outputId": "3d318588-29be-4056-df53-08beac634e81"
      },
      "id": "8NLOhfDFgh-h",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4368\n",
            "Epoch [2/10], Loss: 0.4091\n",
            "Epoch [3/10], Loss: 0.4067\n",
            "Epoch [4/10], Loss: 0.4058\n",
            "Epoch [5/10], Loss: 0.4054\n",
            "Epoch [6/10], Loss: 0.4053\n",
            "Epoch [7/10], Loss: 0.4052\n",
            "Epoch [8/10], Loss: 0.4052\n",
            "Epoch [9/10], Loss: 0.4050\n",
            "Epoch [10/10], Loss: 0.4052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f",
      "metadata": {
        "id": "26b25b5b-0ed2-4bf0-8350-601812eb057f"
      },
      "source": [
        "## 74. モデルの評価\n",
        "\n",
        "問題73で学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for item in dev_tensor_data:\n",
        "        input_ids = item['input_ids'].unsqueeze(0)\n",
        "        label = item['label'].squeeze()\n",
        "        output = model(input_ids).squeeze()\n",
        "        predicted = (output >= 0.5).float()\n",
        "        correct += (predicted == label).sum().item()\n",
        "        total += 1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Development Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "laF14666nBXy",
        "outputId": "893bbbfd-aa89-4de5-8602-f433d080ddf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "laF14666nBXy",
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Development Accuracy: 0.7775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 開発セット用 Dataset & DataLoader\n",
        "dev_dataset = SSTDataset(dev_tensor_data)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=1, shuffle=False, collate_fn=collate_no_padding)\n",
        "\n",
        "# 評価モード\n",
        "model.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input_ids, labels in dev_loader:\n",
        "        # バッチ次元を追加: [seq_len] → [1, seq_len]\n",
        "        input_ids = input_ids.unsqueeze(0)\n",
        "        outputs = model(input_ids).squeeze()\n",
        "        predicted = (outputs >= 0.5).float()  # 0.5 を閾値として 0 or 1 に変換\n",
        "        labels = labels.squeeze()\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += 1 # バッチサイズ1なので +1\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Development Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0iOqBgh_Y8db",
        "outputId": "f5f01003-7b0e-4b45-cc74-7192235cf9de"
      },
      "id": "0iOqBgh_Y8db",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Development Accuracy: 0.7764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "O08V9g0mcJwe",
      "metadata": {
        "id": "O08V9g0mcJwe"
      },
      "source": [
        "## 75. パディング\n",
        "\n",
        "複数の事例が与えられたとき、これらをまとめて一つのテンソル・オブジェクトで表現する関数`collate`を実装せよ。与えられた複数の事例のトークン列の長さが異なるときは、トークン列の長さが最も長いものに揃え、0番のトークンIDでパディングをせよ。さらに、トークン列の長さが長いものから順に、事例を並び替えよ。\n",
        "\n",
        "例えば、訓練データセットの冒頭の4事例が次のように表されているとき、\n",
        "\n",
        "```\n",
        "[{'text': 'hide new secretions from the parental units',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([  5785,     66, 113845,     18,     12,  15095,   1594])},\n",
        " {'text': 'contains no wit , only labored gags',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([ 3475,    87, 15888,    90, 27695, 42637])},\n",
        " {'text': 'that loves its characters and communicates something rather beautiful about human nature',\n",
        "  'label': tensor([1.]),\n",
        "  'input_ids': tensor([    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,  1964])},\n",
        " {'text': 'remains utterly satisfied to remain the same throughout',\n",
        "  'label': tensor([0.]),\n",
        "  'input_ids': tensor([  987, 14528,  4941,   873,    12,   208,   898])}]\n",
        "```\n",
        "\n",
        "`collate`関数を通した結果は以下のようになることが想定される。\n",
        "\n",
        "```\n",
        "{'input_ids': tensor([\n",
        "    [     4,   5053,     45,   3305,  31647,    348,    904,   2815,     47,   1276,   1964],\n",
        "    [  5785,     66, 113845,     18,     12,  15095,   1594,      0,      0,      0,      0],\n",
        "    [   987,  14528,   4941,    873,     12,    208,    898,      0,      0,      0,      0],\n",
        "    [  3475,     87,  15888,     90,  27695,  42637,      0,      0,      0,      0,      0]]),\n",
        " 'label': tensor([\n",
        "    [1.],\n",
        "    [0.],\n",
        "    [0.],\n",
        "    [0.]])}\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_batch(batch):\n",
        "    # バッチ内の要素を input_ids と label に分割\n",
        "    input_ids_list, label_list = zip(*batch)\n",
        "\n",
        "    # 各 input_ids の長さを取得\n",
        "    lengths = [len(x) for x in input_ids_list]\n",
        "\n",
        "    # 長さで降順ソートしたインデックスを取得\n",
        "    sorted_indices = sorted(range(len(lengths)), key=lambda i: -lengths[i])\n",
        "\n",
        "    # ソート済みの input_ids と labels を作成\n",
        "    sorted_input_ids = [input_ids_list[i] for i in sorted_indices]\n",
        "    sorted_labels = [label_list[i] for i in sorted_indices]\n",
        "\n",
        "    # パディング（最大長さに合わせて0で埋める）\n",
        "    padded_input_ids = nn.utils.rnn.pad_sequence(sorted_input_ids, batch_first=True, padding_value=0)\n",
        "    labels = torch.stack(sorted_labels)\n",
        "\n",
        "    return {'input_ids': padded_input_ids, 'label': labels}\n"
      ],
      "metadata": {
        "id": "1l4wIG2ll2KG"
      },
      "id": "1l4wIG2ll2KG",
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_batch = [(item['input_ids'], item['label']) for item in train_tensor_data[:4]]\n",
        "print(collate_batch(sample_batch))"
      ],
      "metadata": {
        "id": "V6JPQayCt1wU",
        "outputId": "4b9167f9-b588-4ded-a277-289b62f9159e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "V6JPQayCt1wU",
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    4,  5053,    45,  3305, 31647,   348,   904,  2815,    47,  1276,\n",
            "          1964],\n",
            "        [  987, 14528,  4941,   873,    12,   208,   898,     0,     0,     0,\n",
            "             0],\n",
            "        [ 5785,    66,    18,    12, 15095,  1594,     0,     0,     0,     0,\n",
            "             0],\n",
            "        [ 3475,    87, 15888,    90, 27695, 42637,     0,     0,     0,     0,\n",
            "             0]]), 'label': tensor([[1.],\n",
            "        [0.],\n",
            "        [0.],\n",
            "        [0.]])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9NzvuZ-5ebDU",
      "metadata": {
        "id": "9NzvuZ-5ebDU"
      },
      "source": [
        "## 76. ミニバッチ学習\n",
        "\n",
        "問題75のパディングの処理を活用して、ミニバッチでモデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# 学習用データ（辞書のリスト）を PyTorch Dataset に変換\n",
        "class SSTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]['input_ids'], self.data[idx]['label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "#PyTorch 用に変換\n",
        "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "# モデル（embeddingはすでに提供されたもの）\n",
        "model2 = MeanEmbeddingLogisticRegression(embedding_tensor)\n",
        "\n",
        "# 埋め込みは凍結する\n",
        "model2.embedding.weight.requires_grad = False\n",
        "\n",
        "# ハイパーパラメータ\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 1e-3\n",
        "\n",
        "# 損失関数と最適化手法\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model2.parameters(), lr=lr)\n",
        "\n",
        "# DataLoader の作成\n",
        "train_dataset = SSTDataset(train_tensor_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    model2.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        labels = batch['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model2(input_ids).view(-1)\n",
        "        loss = criterion(outputs, labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oeZDf7MclyuM",
        "outputId": "3b0b1d10-923e-4686-c0ce-8b10016da24d"
      },
      "id": "oeZDf7MclyuM",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5485\n",
            "Epoch [2/10], Loss: 0.4611\n",
            "Epoch [3/10], Loss: 0.4372\n",
            "Epoch [4/10], Loss: 0.4259\n",
            "Epoch [5/10], Loss: 0.4196\n",
            "Epoch [6/10], Loss: 0.4155\n",
            "Epoch [7/10], Loss: 0.4128\n",
            "Epoch [8/10], Loss: 0.4109\n",
            "Epoch [9/10], Loss: 0.4093\n",
            "Epoch [10/10], Loss: 0.4083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 開発セット用 Dataset & DataLoader\n",
        "dev_dataset = SSTDataset(dev_tensor_data)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# 評価モード\n",
        "model2.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in dev_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        labels = batch['label']\n",
        "        outputs = model2(input_ids).view(-1)\n",
        "        predicted = (outputs >= 0.5).float()\n",
        "        correct += (predicted == labels.view(-1)).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Development Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNJDeFZZlt9K",
        "outputId": "b30853a4-fe56-4190-eba1-fd52e359752c"
      },
      "id": "pNJDeFZZlt9K",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Development Accuracy: 0.7718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RUbjivUTejxn",
      "metadata": {
        "id": "RUbjivUTejxn"
      },
      "source": [
        "## 77. GPU上での学習\n",
        "\n",
        "問題76のモデル学習をGPU上で実行せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# 学習用データ（辞書のリスト）を PyTorch Dataset に変換\n",
        "class SSTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]['input_ids'], self.data[idx]['label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "#PyTorch 用に変換\n",
        "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "# モデル（embeddingはすでに提供されたもの）\n",
        "model2 = MeanEmbeddingLogisticRegression(embedding_tensor)\n",
        "\n",
        "# 埋め込みは凍結する\n",
        "model2.embedding.weight.requires_grad = False\n",
        "\n",
        "# ハイパーパラメータ\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 1e-3\n",
        "\n",
        "# 損失関数と最適化手法\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model2.parameters(), lr=lr)\n",
        "\n",
        "# DataLoader の作成\n",
        "train_dataset = SSTDataset(train_tensor_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    model2.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        labels = batch['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model2(input_ids).view(-1)\n",
        "        loss = criterion(outputs, labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "HuOsSHduZIU7",
        "outputId": "d4e23ff6-2fcb-4ca3-abe8-a28cdc2e76b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "HuOsSHduZIU7",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.5487\n",
            "Epoch [2/10], Loss: 0.4610\n",
            "Epoch [3/10], Loss: 0.4369\n",
            "Epoch [4/10], Loss: 0.4257\n",
            "Epoch [5/10], Loss: 0.4194\n",
            "Epoch [6/10], Loss: 0.4154\n",
            "Epoch [7/10], Loss: 0.4126\n",
            "Epoch [8/10], Loss: 0.4107\n",
            "Epoch [9/10], Loss: 0.4093\n",
            "Epoch [10/10], Loss: 0.4082\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 開発セット用 Dataset & DataLoader\n",
        "dev_dataset = SSTDataset(dev_tensor_data)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# 評価モード\n",
        "model2.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in dev_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        labels = batch['label']\n",
        "        outputs = model2(input_ids).view(-1)\n",
        "        predicted = (outputs >= 0.5).float()\n",
        "        correct += (predicted == labels.view(-1)).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Development Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "W-aFGEUAZ2ks",
        "outputId": "bcd7b266-6bf6-4b52-edf1-eb1a216f6f66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "W-aFGEUAZ2ks",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Development Accuracy: 0.7718\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZUY1PsD-eplq",
      "metadata": {
        "id": "ZUY1PsD-eplq"
      },
      "source": [
        "## 78. 単語埋め込みのファインチューニング\n",
        "\n",
        "問題77の学習において、単語埋め込みのパラメータも同時に更新するファインチューニングを導入せよ。また、学習したモデルの開発セットにおける正解率を求めよ。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# 学習用データ（辞書のリスト）を PyTorch Dataset に変換\n",
        "class SSTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]['input_ids'], self.data[idx]['label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "#PyTorch 用に変換\n",
        "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "# モデル（embeddingはすでに提供されたもの）\n",
        "model3 = MeanEmbeddingLogisticRegression(embedding_tensor)\n",
        "\n",
        "# 埋め込みは凍結する\n",
        "#model3.embedding.weight.requires_grad = True\n",
        "model3.embedding.weight.requires_grad = False\n",
        "model3.embedding.weight[:5000].requires_grad = True  # 頻出上位5000語のみ更新\n",
        "\n",
        "# ハイパーパラメータ\n",
        "batch_size = 8\n",
        "epochs = 10\n",
        "lr = 1e-3\n",
        "\n",
        "# 損失関数と最適化手法\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model3.parameters(), lr=lr)\n",
        "\n",
        "# DataLoader の作成\n",
        "train_dataset = SSTDataset(train_tensor_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    model3.train()\n",
        "    for batch in train_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        labels = batch['label']\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model3(input_ids).view(-1)\n",
        "        loss = criterion(outputs, labels.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "Ik3XgT-YgFLL",
        "outputId": "37891f2f-60a3-46f2-c466-320da465e1e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Ik3XgT-YgFLL",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4744\n",
            "Epoch [2/10], Loss: 0.4195\n",
            "Epoch [3/10], Loss: 0.4115\n",
            "Epoch [4/10], Loss: 0.4083\n",
            "Epoch [5/10], Loss: 0.4065\n",
            "Epoch [6/10], Loss: 0.4055\n",
            "Epoch [7/10], Loss: 0.4050\n",
            "Epoch [8/10], Loss: 0.4044\n",
            "Epoch [9/10], Loss: 0.4042\n",
            "Epoch [10/10], Loss: 0.4039\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 開発セット用 Dataset & DataLoader\n",
        "dev_dataset = SSTDataset(dev_tensor_data)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# 評価モード\n",
        "model3.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in dev_loader:\n",
        "        input_ids = batch['input_ids']\n",
        "        labels = batch['label']\n",
        "        outputs = model3(input_ids).view(-1)\n",
        "        predicted = (outputs >= 0.5).float()\n",
        "        correct += (predicted == labels.view(-1)).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Development Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "Z_DnM_F4gUyz",
        "outputId": "1b58bbcc-1f94-4be3-debe-d031cc9edc9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "Z_DnM_F4gUyz",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Development Accuracy: 0.7867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jVAdWIq0evKR",
      "metadata": {
        "id": "jVAdWIq0evKR"
      },
      "source": [
        "## 79. アーキテクチャの変更\n",
        "\n",
        "ニューラルネットワークのアーキテクチャを自由に変更し、モデルを学習せよ。また、学習したモデルの開発セットにおける正解率を求めよ。例えば、テキストの特徴ベクトル（単語埋め込みの平均ベクトル）に対して多層のニューラルネットワークを通したり、畳み込みニューラルネットワーク（CNN; Convolutional Neural Network）や再帰型ニューラルネットワーク（RNN; Recurrent Neural Network）などのモデルの学習に挑戦するとよい。"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "# 平均埋め込み + MLP モデル定義\n",
        "class MeanEmbeddingMLP(nn.Module):\n",
        "    def __init__(self, embedding_weights, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        vocab_size, embedding_dim = embedding_weights.shape\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.embedding.weight.data.copy_(embedding_weights.clone().detach())\n",
        "        self.embedding.weight.requires_grad = False  # ファインチューニングする\n",
        "\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(embedding_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids):  # input_ids: [batch_size, seq_len]\n",
        "        x = self.embedding(input_ids)                      # [batch_size, seq_len, emb_dim]\n",
        "        x = x.mean(dim=1)                                  # [batch_size, emb_dim]\n",
        "        out = self.mlp(x).squeeze(1)                       # [batch_size]\n",
        "        return out\n",
        "\n",
        "# 学習用データ（辞書のリスト）を PyTorch Dataset に変換\n",
        "class SSTDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]['input_ids'], self.data[idx]['label']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "#PyTorch 用に変換\n",
        "embedding_tensor = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
        "# モデル（embeddingはすでに提供されたもの）\n",
        "model4 = MeanEmbeddingMLP(embedding_tensor)\n",
        "\n",
        "# ハイパーパラメータ\n",
        "batch_size = 64\n",
        "epochs = 10\n",
        "lr = 1e-3\n",
        "\n",
        "# 損失関数と最適化手法\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model4.parameters(), lr=lr)\n",
        "\n",
        "# DataLoader の作成\n",
        "train_dataset = SSTDataset(train_tensor_data)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "# 学習ループ\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0.0\n",
        "    model4.train()\n",
        "\n",
        "    for input_ids, labels in train_loader:\n",
        "      optimizer.zero_grad()\n",
        "      outputs = model4(input_ids).view(-1)\n",
        "      loss = criterion(outputs, labels.view(-1))\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      total_loss += loss.item() * input_ids.size(0)\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {avg_loss:.4f}\")"
      ],
      "metadata": {
        "id": "e8q_EGCynY4g",
        "outputId": "225aae8b-8968-44f1-9e18-9901a92bbed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "e8q_EGCynY4g",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.4671\n",
            "Epoch [2/10], Loss: 0.3907\n",
            "Epoch [3/10], Loss: 0.3768\n",
            "Epoch [4/10], Loss: 0.3681\n",
            "Epoch [5/10], Loss: 0.3616\n",
            "Epoch [6/10], Loss: 0.3566\n",
            "Epoch [7/10], Loss: 0.3511\n",
            "Epoch [8/10], Loss: 0.3450\n",
            "Epoch [9/10], Loss: 0.3398\n",
            "Epoch [10/10], Loss: 0.3330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 開発セット用 Dataset & DataLoader\n",
        "dev_dataset = SSTDataset(dev_tensor_data)\n",
        "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# 評価モード\n",
        "model4.eval()\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for input_ids, labels in dev_loader:  # ← タプルとして unpack\n",
        "        outputs = model4(input_ids).view(-1)\n",
        "        predicted = (outputs >= 0.5).float()\n",
        "        correct += (predicted == labels.view(-1)).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Development Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "id": "j8ufDzeDsUkG",
        "outputId": "fdcaec80-6581-482e-c28b-fa8e9d53134c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "j8ufDzeDsUkG",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Development Accuracy: 0.7913\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}